{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ES-SSM (Elastic Spectral State Space Model) on PG19\n",
    "# ============================================================\n",
    "\n",
    "import os, sys, math, time, random, hashlib, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0,1\")\n",
    "if \"torch\" in sys.modules:\n",
    "    raise RuntimeError(\n",
    "        \"torch is already imported in this kernel. Restart the kernel, then run this cell again \"\n",
    "        \"so CUDA_VISIBLE_DEVICES takes effect.\"\n",
    "    )\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, IterableDataset, get_worker_info, Dataset\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "# 1) CONFIG (PG19)\n",
    "\n",
    "CONFIG = {\n",
    "    # Repro\n",
    "    \"seed\": 86,\n",
    "    \"deterministic\": False,\n",
    "\n",
    "    # Data\n",
    "    \"dataset_name\": \"pg19\",\n",
    "    \"vocab_size\": 258,            \n",
    "    \"seq_len\": 4096,            \n",
    "    \"num_workers\": 2,\n",
    "    \"batch_size\": 4,                \n",
    "    \"steps_per_epoch\": 4000,       \n",
    "    \"pin_memory\": True,\n",
    "    \"train_shuffle_buffer_books\": 512,\n",
    "\n",
    "\n",
    "    \"eval_cache_dir\": \"./eval_cache_pg19\",\n",
    "    \"eval_cache_segments\": 1024,  \n",
    "    \"eval_cache_shuffle_books\": True,\n",
    "    \"eval_cache_shuffle_buffer_books\": 4096,\n",
    "\n",
    "    # Regularization\n",
    "    \"token_dropout_prob\": 0.0,     \n",
    "    \"dropout\": 0.10,\n",
    "    \"stoch_depth\": 0.05,\n",
    "\n",
    "    # Model (ES-SSM)\n",
    "    \"d_model\": 256,\n",
    "    \"n_layers\": 8,\n",
    "    \"K_max\": 32,\n",
    "    \"K_chunk\": 8,\n",
    "\n",
    "    # Training\n",
    "    \"epochs\": 300,  \n",
    "    \"warmup_epochs\": 2,\n",
    "    \"grad_accum_steps\": 4,          \n",
    "\n",
    "    \"lr\": 2e-4,\n",
    "    \"min_lr_ratio\": 0.10,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "\n",
    "    \"weight_decay\": 0.03,\n",
    "    \"adam_betas\": (0.9, 0.95),\n",
    "    \"adam_eps\": 1e-8,\n",
    "\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"use_amp\": True,\n",
    "    \"ema_decay\": 0.999,\n",
    "\n",
    "    # Budget dropout\n",
    "    \"budget_enabled\": True,\n",
    "    \"budget_k_min\": 2,\n",
    "    \"budget_full_every\": 8,       \n",
    "    \"budget_ks\": [2, 3, 4, 6, 8, 12, 16, 24, 32],\n",
    "    \"budget_bias_to_large\": 0.6,    \n",
    "\n",
    "    # Evaluation schedule\n",
    "    \"eval_batches_train\": 25,       \n",
    "    \"test_every\": 3,\n",
    "    \"eval_curve_every\": 3,\n",
    "    \"eval_batches_final\": 256,    \n",
    "\n",
    "    # Early stopping\n",
    "    \"early_stop_patience\": 10,\n",
    "    \"early_stop_min_delta\": 1e-4,\n",
    "\n",
    "    # Save paths \n",
    "    \"weights_dir\": \"./weights_esssm\",\n",
    "    \"run_name\": \"pg19_bytes_esssm_dp_L4096\",\n",
    "    \"resume\": False,\n",
    "    \"fresh_start\": True,\n",
    "    \"save_last_every_epoch\": 1,\n",
    "\n",
    "    # Hankel filter cache\n",
    "    \"filter_cache_dir\": \"./cache_filters\",\n",
    "    \"filter_iters\": 70,\n",
    "    \"filter_oversample\": 8,\n",
    "    \"filter_tol\": 1e-6,\n",
    "\n",
    "    # Numeric stability\n",
    "    \"rms_eps\": 1e-6,\n",
    "\n",
    "\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# 2) Utilities\n",
    "def print0(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "\n",
    "def unwrap_model(m: nn.Module) -> nn.Module:\n",
    "    return m.module if isinstance(m, nn.DataParallel) else m\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "if CONFIG[\"deterministic\"]:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "Path(CONFIG[\"weights_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"filter_cache_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CONFIG[\"eval_cache_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "AMP_DEVICE = \"cuda\" if CONFIG[\"device\"] == \"cuda\" else \"cpu\"\n",
    "\n",
    "print0(f\"[Device] {CONFIG['device']} | CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "print0(f\"[Run] L={CONFIG['seq_len']} | batch={CONFIG['batch_size']} | grad_accum={CONFIG['grad_accum_steps']} | AMP={CONFIG['use_amp']}\")\n",
    "print0(f\"[Model] d={CONFIG['d_model']} | layers={CONFIG['n_layers']} | K_max={CONFIG['K_max']} | K_chunk={CONFIG['K_chunk']}\")\n",
    "print0(f\"[Data] PG19 via HF datasets: {CONFIG['dataset_name']} | streaming=True\")\n",
    "\n",
    "\n",
    "\n",
    "# 3) Byte tokenization\n",
    "def _bytes_to_tokens(b: bytes) -> torch.Tensor:\n",
    "    fb = getattr(torch, \"frombuffer\", None)\n",
    "    if fb is None:\n",
    "        t = torch.tensor(list(b), dtype=torch.uint8)\n",
    "    else:\n",
    "        t = torch.frombuffer(bytearray(b), dtype=torch.uint8)\n",
    "    return t.to(torch.int64).add_(2) \n",
    "\n",
    "\n",
    "\n",
    "# 4) PG19 streaming dataset for TRAIN\n",
    "class PG19ByteLMIterable(IterableDataset):\n",
    "\n",
    "    def __init__(self, split: str, seq_len: int, seed: int, shuffle_books: bool, shuffle_buffer_books: int):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.seq_len = int(seq_len)\n",
    "        self.seg_len = int(seq_len) + 1\n",
    "        self.seed = int(seed)\n",
    "        self.shuffle_books = bool(shuffle_books)\n",
    "        self.shuffle_buffer_books = int(shuffle_buffer_books)\n",
    "\n",
    "    def _make_stream(self):\n",
    "        return load_dataset(CONFIG[\"dataset_name\"], split=self.split, streaming=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        wi = get_worker_info()\n",
    "        worker_id = 0 if wi is None else wi.id\n",
    "        num_workers = 1 if wi is None else wi.num_workers\n",
    "\n",
    "        ds0 = self._make_stream()\n",
    "        n_shards = getattr(ds0, \"n_shards\", None)\n",
    "        if n_shards is None:\n",
    "            n_shards = getattr(ds0, \"num_shards\", None)\n",
    "        if n_shards is None:\n",
    "            n_shards = 1\n",
    "        n_shards = int(n_shards)\n",
    "\n",
    "        eff_workers = min(num_workers, n_shards)\n",
    "        if wi is not None and worker_id >= eff_workers:\n",
    "            return\n",
    "\n",
    "        ds = self._make_stream()\n",
    "        if eff_workers > 1:\n",
    "            ds = ds.shard(num_shards=eff_workers, index=worker_id)\n",
    "\n",
    "        if self.shuffle_books:\n",
    "            ds = ds.shuffle(seed=self.seed + 13 * worker_id, buffer_size=self.shuffle_buffer_books)\n",
    "\n",
    "        buf = bytearray()\n",
    "        stride = self.seq_len\n",
    "\n",
    "        while True:\n",
    "            for ex in ds:\n",
    "                text = ex.get(\"text\", None)\n",
    "                if not text:\n",
    "                    continue\n",
    "                b = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "                if not b:\n",
    "                    continue\n",
    "\n",
    "                buf.extend(b)\n",
    "                buf.extend(b\"\\n\")\n",
    "\n",
    "                while len(buf) >= self.seg_len:\n",
    "                    chunk = bytes(buf[: self.seg_len])\n",
    "                    del buf[: stride]\n",
    "                    yield _bytes_to_tokens(chunk)  # (L+1,)\n",
    "\n",
    "            ds = self._make_stream()\n",
    "            if eff_workers > 1:\n",
    "                ds = ds.shard(num_shards=eff_workers, index=worker_id)\n",
    "            if self.shuffle_books:\n",
    "                ds = ds.shuffle(seed=self.seed + 13 * worker_id, buffer_size=self.shuffle_buffer_books)\n",
    "\n",
    "def collate_fixed(batch):\n",
    "    return torch.stack(batch, dim=0)  # (B, L+1)\n",
    "\n",
    "\n",
    "\n",
    "# 5) Eval cache\n",
    "@torch.no_grad()\n",
    "def _token_unigram_bpb(tokens_u16: torch.Tensor) -> float:\n",
    "    t = tokens_u16.to(torch.long)\n",
    "    x = t[:, :-1].reshape(-1)\n",
    "    y = t[:, 1:].reshape(-1)\n",
    "    V = int(CONFIG[\"vocab_size\"])\n",
    "    counts = torch.bincount(x, minlength=V).float()\n",
    "    probs = counts / counts.sum().clamp_min(1.0)\n",
    "    py = probs[y].clamp_min(1e-12)\n",
    "    nll = (-torch.log(py)).mean().item()  # nats\n",
    "    return float(nll / math.log(2.0))\n",
    "\n",
    "def _seg_hash_u16(seg_u16: torch.Tensor) -> str:\n",
    "    seg_u8 = seg_u16.contiguous().view(torch.uint8).cpu()\n",
    "    b = bytes(seg_u8.tolist())\n",
    "    return hashlib.sha1(b).hexdigest()\n",
    "\n",
    "def _stream_n_shards(split: str) -> int:\n",
    "    d = load_dataset(CONFIG[\"dataset_name\"], split=split, streaming=True)\n",
    "    ns = getattr(d, \"n_shards\", None)\n",
    "    if ns is None:\n",
    "        ns = getattr(d, \"num_shards\", None)\n",
    "    return int(ns) if ns is not None else 1\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_eval_cache(split: str, out_path: Path, num_segments: int, seed: int,\n",
    "                     shuffle_books: bool, shuffle_buffer_books: int, seq_len: int):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        print0(f\"[EvalCache] Using existing cache: {out_path}\")\n",
    "        ck = torch.load(out_path, map_location=\"cpu\")\n",
    "        return ck[\"tokens_u16\"]\n",
    "\n",
    "    print0(f\"[EvalCache] Building {split} cache: segments={num_segments} -> {out_path}\")\n",
    "    ds = PG19ByteLMIterable(\n",
    "        split=split,\n",
    "        seq_len=seq_len,\n",
    "        seed=seed,\n",
    "        shuffle_books=shuffle_books,\n",
    "        shuffle_buffer_books=shuffle_buffer_books\n",
    "    )\n",
    "\n",
    "    tokens = []\n",
    "    it = iter(ds)\n",
    "    for i in range(int(num_segments)):\n",
    "        seg = next(it)  # (L+1,) int64\n",
    "        tokens.append(seg.to(torch.uint16).cpu())\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print0(f\"  cached {i+1}/{num_segments} segments ...\")\n",
    "\n",
    "    tokens_u16 = torch.stack(tokens, dim=0).contiguous()  # (N, L+1) uint16\n",
    "    torch.save(\n",
    "        {\n",
    "            \"tokens_u16\": tokens_u16,\n",
    "            \"split\": split,\n",
    "            \"seq_len\": int(seq_len),\n",
    "            \"seed\": int(seed),\n",
    "            \"shuffle_books\": bool(shuffle_books),\n",
    "            \"num_segments\": int(num_segments),\n",
    "        },\n",
    "        out_path\n",
    "    )\n",
    "    print0(f\"[EvalCache] Saved {split} -> {out_path}\")\n",
    "    return tokens_u16\n",
    "\n",
    "class CachedSegmentsDataset(Dataset):\n",
    "    def __init__(self, tokens_u16: torch.Tensor):\n",
    "        super().__init__()\n",
    "        assert tokens_u16.dtype == torch.uint16\n",
    "        self.tokens_u16 = tokens_u16\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokens_u16.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens_u16[idx].to(torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# 6) Hankel top-K via FFT subspace iteration\n",
    "\n",
    "def hankel_toeplitz_embed_g(L: int, eps: float = 1e-6) -> torch.Tensor:\n",
    "    k = torch.arange(0, 2 * L - 1, dtype=torch.float32)\n",
    "    s = k + 2.0\n",
    "    a = 2.0 / (s**3 - s + eps)\n",
    "    c = a[L - 1 :]\n",
    "    g = torch.cat([c, torch.zeros(1), a[: L - 1]])  # len 2L\n",
    "    return g.contiguous()\n",
    "\n",
    "@torch.no_grad()\n",
    "def hankel_mv_batch_fft(g_f: torch.Tensor, X: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    L, p = X.shape\n",
    "    n = 2 * L\n",
    "    X_rev = torch.flip(X, dims=[0])\n",
    "    X_pad = torch.zeros((p, n), dtype=torch.float32, device=X.device)\n",
    "    X_pad[:, :L] = X_rev.T\n",
    "    X_f = torch.fft.rfft(X_pad, dim=-1)\n",
    "    Y_pad = torch.fft.irfft(X_f * g_f.unsqueeze(0), n=n, dim=-1)\n",
    "    return Y_pad[:, :L].T\n",
    "\n",
    "@torch.no_grad()\n",
    "def hankel_topk_subspace(L: int, K: int, cache_path: str,\n",
    "                         iters: int, oversample: int, tol: float):\n",
    "\n",
    "    cache_path = str(cache_path)\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "    if os.path.exists(cache_path):\n",
    "        ck = torch.load(cache_path, map_location=\"cpu\")\n",
    "        return ck[\"sigma\"].float(), ck[\"phi\"].float()\n",
    "\n",
    "    print0(f\"[Filters] Computing Hankel top-K (FFT subspace): L={L}, K={K}, iters={iters}, oversample={oversample}\")\n",
    "    device = \"cpu\"\n",
    "    g = hankel_toeplitz_embed_g(L).to(device)\n",
    "    g_f = torch.fft.rfft(g)\n",
    "\n",
    "    p = K + oversample\n",
    "    Q = torch.randn(L, p, dtype=torch.float32, device=device)\n",
    "    Q, _ = torch.linalg.qr(Q, mode=\"reduced\")\n",
    "\n",
    "    last = None\n",
    "    for t in range(1, iters + 1):\n",
    "        ZQ = hankel_mv_batch_fft(g_f, Q)\n",
    "        Q, _ = torch.linalg.qr(ZQ, mode=\"reduced\")\n",
    "\n",
    "        if (t % 10 == 0) or (t == iters):\n",
    "            AQ = hankel_mv_batch_fft(g_f, Q)\n",
    "            Bm = Q.T @ AQ\n",
    "            w, V = torch.linalg.eigh(Bm)\n",
    "            idx = torch.argsort(w, descending=True)\n",
    "            w = w[idx]; V = V[:, idx]\n",
    "            Q = Q @ V\n",
    "\n",
    "            Qk = Q[:, :K]\n",
    "            AQk = hankel_mv_batch_fft(g_f, Qk)\n",
    "            R = AQk - Qk * w[:K].unsqueeze(0)\n",
    "            rel = (R.norm(dim=0) / (w[:K].abs() + 1e-12))\n",
    "            mx = float(rel.max().item())\n",
    "            msg = f\"[Filters] iter {t:03d}/{iters} | max_rel_res(topK)={mx:.3e}\"\n",
    "            if last is not None:\n",
    "                msg += f\" | delta={last - mx:+.2e}\"\n",
    "            print0(msg)\n",
    "            last = mx\n",
    "            if mx < tol:\n",
    "                print0(\"[Filters] Converged.\")\n",
    "                break\n",
    "\n",
    "    AQ = hankel_mv_batch_fft(g_f, Q)\n",
    "    Bm = Q.T @ AQ\n",
    "    w, V = torch.linalg.eigh(Bm)\n",
    "    idx = torch.argsort(w, descending=True)\n",
    "    w = w[idx]; V = V[:, idx]\n",
    "    Q = Q @ V\n",
    "\n",
    "    sigma = w[:K].contiguous()\n",
    "    phi = Q[:, :K].contiguous()\n",
    "\n",
    "    torch.save({\"sigma\": sigma.cpu(), \"phi\": phi.cpu()}, cache_path)\n",
    "    print0(f\"[Filters] Saved torch cache -> {cache_path}\")\n",
    "    return sigma.cpu(), phi.cpu()\n",
    "\n",
    "\n",
    "\n",
    "# 7) ES-SSM layer\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if (not self.training) or self.drop_prob <= 0.0:\n",
    "            return x\n",
    "        keep = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        mask = (torch.rand(shape, device=x.device, dtype=x.dtype) < keep)\n",
    "        return x * mask / keep\n",
    "\n",
    "def rms_rescale_logits(s: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "\n",
    "    K = s.shape[-1]\n",
    "    norm = torch.linalg.vector_norm(s.float(), ord=2, dim=-1, keepdim=True).clamp_min(eps)\n",
    "    return s * (float(K) ** 0.5) / norm\n",
    "\n",
    "class ESSSM_Layer(nn.Module):\n",
    "    def __init__(self, d_model: int, L: int, K_max: int, K_chunk: int, drop_path: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.K_max = int(K_max)\n",
    "        self.L = int(L)\n",
    "        self.K_chunk = int(K_chunk)\n",
    "\n",
    "        cache_dir = Path(CONFIG[\"filter_cache_dir\"])\n",
    "        cache_path = cache_dir / f\"hankel_topk_L{L}_K{K_max}.pt\"\n",
    "\n",
    "        sigma, phi = hankel_topk_subspace(\n",
    "            L=L, K=K_max, cache_path=str(cache_path),\n",
    "            iters=int(CONFIG[\"filter_iters\"]),\n",
    "            oversample=int(CONFIG[\"filter_oversample\"]),\n",
    "            tol=float(CONFIG[\"filter_tol\"]),\n",
    "        )\n",
    "        self.register_buffer(\"phi\", phi)       # (L, K_max)\n",
    "        self.register_buffer(\"sigma\", sigma)   # (K_max,)\n",
    "\n",
    "        # FFT(phi_k) for convolution\n",
    "        n = 2 * L\n",
    "        with torch.no_grad():\n",
    "            phi_f = torch.fft.rfft(phi.T.contiguous().float(), n=n, dim=-1)   # (K, F) complex\n",
    "            phi_f_ri = torch.view_as_real(phi_f).contiguous()                 # (K, F, 2)\n",
    "        self.register_buffer(\"phi_f_ri\", phi_f_ri)\n",
    "\n",
    "        # Mixing matrices M_k\n",
    "        self.M_phi = nn.Parameter(torch.randn(K_max, d_model, d_model) * 1e-3)\n",
    "\n",
    "        # Gate MLP -> logits s_k(t)\n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, K_max),\n",
    "        )\n",
    "        nn.init.constant_(self.selector[-1].bias, -2.0)\n",
    "\n",
    "        # D u(t)\n",
    "        self.D = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, u, K_runtime: int = None):\n",
    "        \"\"\"\n",
    "        u: (B, L, D)\n",
    "        K_runtime: runtime budget K <= K_max\n",
    "        \"\"\"\n",
    "        B, L, D = u.shape\n",
    "        assert L == self.L, f\"Expected L={self.L}, got {L}\"\n",
    "        K_max = self.K_max\n",
    "        n = 2 * L\n",
    "\n",
    "        # logits s(t) over all channels\n",
    "        s_all = self.selector(u)  # (B, L, K_max)\n",
    "\n",
    "        # budget K\n",
    "        if K_runtime is None:\n",
    "            K = K_max\n",
    "        else:\n",
    "            K = int(K_runtime)\n",
    "            K = max(int(CONFIG.get(\"budget_k_min\", 2)), min(K_max, K))\n",
    "\n",
    "        # active prefix 1..K\n",
    "        s = s_all[:, :, :K]\n",
    "        s_tilde = rms_rescale_logits(s, eps=float(CONFIG[\"rms_eps\"]))\n",
    "        w = torch.softmax(s_tilde.float(), dim=-1).to(dtype=u.dtype)  # alpha_k(t), (B, L, K)\n",
    "\n",
    "        # sigma^(1/4)\n",
    "        sigma_scale = torch.clamp(self.sigma[:K], min=1e-12).pow(0.25).to(dtype=u.dtype)  # (K,)\n",
    "\n",
    "        # FFT(u) in float32\n",
    "        with torch.amp.autocast(device_type=AMP_DEVICE, enabled=False):\n",
    "            u_f = torch.fft.rfft(u.float().permute(0, 2, 1), n=n, dim=-1)  # (B, D, F) complex\n",
    "            u_f_ri = torch.view_as_real(u_f).contiguous()                  # (B, D, F, 2)\n",
    "\n",
    "        y_acc = torch.zeros((B, L, D), device=u.device, dtype=u.dtype)\n",
    "\n",
    "        # chunk over K\n",
    "        for k0 in range(0, K, self.K_chunk):\n",
    "            k1 = min(K, k0 + self.K_chunk)\n",
    "            Kc = k1 - k0\n",
    "\n",
    "            phi_f_chunk = self.phi_f_ri[k0:k1]            # (Kc, F, 2)\n",
    "            M_chunk = self.M_phi[k0:k1]                   # (Kc, D, D)\n",
    "            w_chunk = w[:, :, k0:k1]                      # (B, L, Kc)\n",
    "            s_chunk = sigma_scale[k0:k1]                  # (Kc,)\n",
    "\n",
    "            # Convolution (Phi_k * u) via FFT multiply and iFFT\n",
    "            with torch.amp.autocast(device_type=AMP_DEVICE, enabled=False):\n",
    "                a = u_f_ri.unsqueeze(2)                   # (B, D, 1, F, 2)\n",
    "                b = phi_f_chunk.unsqueeze(0).unsqueeze(0) # (1, 1, Kc, F, 2)\n",
    "\n",
    "                real = a[..., 0] * b[..., 0] - a[..., 1] * b[..., 1]\n",
    "                imag = a[..., 0] * b[..., 1] + a[..., 1] * b[..., 0]\n",
    "\n",
    "                U_f_ri = torch.stack((real, imag), dim=-1).contiguous()  # (B, D, Kc, F, 2)\n",
    "                U_f_c = torch.view_as_complex(U_f_ri)                    # (B, D, Kc, F)\n",
    "                U_time = torch.fft.irfft(U_f_c, n=n, dim=-1)[..., :L]    # (B, D, Kc, L)\n",
    "                U_out = U_time.permute(0, 3, 2, 1).contiguous()          # (B, L, Kc, D)\n",
    "\n",
    "            U_out = U_out.to(dtype=u.dtype)\n",
    "\n",
    "\n",
    "            projected = torch.einsum(\"blkd,kdo->blko\", U_out, M_chunk.to(dtype=u.dtype))\n",
    "            contrib = (projected * s_chunk.view(1, 1, Kc, 1) * w_chunk.unsqueeze(-1)).sum(dim=2)\n",
    "            y_acc = y_acc + contrib\n",
    "\n",
    "        core = y_acc + self.D(u)\n",
    "        return self.drop_path(core), K\n",
    "\n",
    "\n",
    "\n",
    "# 8) Deep LM \n",
    "\n",
    "class DeepESSM_LM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        V = int(CONFIG[\"vocab_size\"])\n",
    "        d = int(CONFIG[\"d_model\"])\n",
    "        L = int(CONFIG[\"seq_len\"])\n",
    "        K_max = int(CONFIG[\"K_max\"])\n",
    "\n",
    "        self.embedding = nn.Embedding(V, d, padding_idx=0)\n",
    "        self.drop = nn.Dropout(float(CONFIG[\"dropout\"]))\n",
    "\n",
    "        sd_max = float(CONFIG[\"stoch_depth\"])\n",
    "        sd_rates = torch.linspace(0.0, sd_max, int(CONFIG[\"n_layers\"])).tolist()\n",
    "\n",
    "        self.ssm_layers = nn.ModuleList([\n",
    "            ESSSM_Layer(d, L, K_max, int(CONFIG[\"K_chunk\"]), drop_path=float(sd_rates[i]))\n",
    "            for i in range(int(CONFIG[\"n_layers\"]))\n",
    "        ])\n",
    "\n",
    "        self.norm1 = nn.ModuleList([nn.LayerNorm(d) for _ in range(int(CONFIG[\"n_layers\"]))])\n",
    "        self.norm2 = nn.ModuleList([nn.LayerNorm(d) for _ in range(int(CONFIG[\"n_layers\"]))])\n",
    "\n",
    "        self.ffn = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d, d * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(float(CONFIG[\"dropout\"])),\n",
    "                nn.Linear(d * 4, d),\n",
    "            )\n",
    "            for _ in range(int(CONFIG[\"n_layers\"]))\n",
    "        ])\n",
    "        self.ffn_drop = nn.ModuleList([DropPath(float(sd_rates[i])) for i in range(int(CONFIG[\"n_layers\"]))])\n",
    "\n",
    "        self.final_norm = nn.LayerNorm(d)\n",
    "        self.lm_head = nn.Linear(d, V, bias=False)\n",
    "\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "        self.lm_head.weight = self.embedding.weight \n",
    "\n",
    "    def forward(self, tokens, K_runtime=None):\n",
    "        x = self.drop(self.embedding(tokens))  # (B, L, d)\n",
    "\n",
    "        K_used = []\n",
    "        for i in range(len(self.ssm_layers)):\n",
    "            y, K = self.ssm_layers[i](self.norm1[i](x), K_runtime=K_runtime)\n",
    "            x = x + y\n",
    "            K_used.append(float(K))\n",
    "            x = x + self.ffn_drop[i](self.ffn[i](self.norm2[i](x)))\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        K_used_t = torch.tensor(K_used, device=logits.device, dtype=torch.float32).unsqueeze(0)\n",
    "        return logits, K_used_t\n",
    "\n",
    "\n",
    "\n",
    "# 9) EMA\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float):\n",
    "        self.decay = float(decay)\n",
    "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items() if v.dtype.is_floating_point}\n",
    "        self.backup = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        sd = model.state_dict()\n",
    "        for k in self.shadow:\n",
    "            v = sd[k]\n",
    "            self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\n",
    "\n",
    "    def apply(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        sd = model.state_dict()\n",
    "        for k in self.shadow:\n",
    "            self.backup[k] = sd[k].detach().clone()\n",
    "            sd[k].copy_(self.shadow[k])\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        if self.backup is None:\n",
    "            return\n",
    "        sd = model.state_dict()\n",
    "        for k in self.backup:\n",
    "            sd[k].copy_(self.backup[k])\n",
    "        self.backup = None\n",
    "\n",
    "\n",
    "\n",
    "# 10) LR schedule\n",
    "\n",
    "def make_warmup_cosine_scheduler(optimizer, total_steps: int, warmup_steps: int, min_lr_ratio: float):\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "        return min_lr_ratio + (1.0 - min_lr_ratio) * cosine\n",
    "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "\n",
    "# 11) Budget dropout\n",
    "class BudgetSampler:\n",
    "    def __init__(self, seed: int):\n",
    "        self.rng = random.Random(seed + 777)\n",
    "        self.cands = sorted(list(CONFIG[\"budget_ks\"]))\n",
    "        if len(self.cands) == 0:\n",
    "            raise ValueError(\"CONFIG['budget_ks'] is empty.\")\n",
    "        self.k_min = max(int(CONFIG[\"budget_k_min\"]), int(self.cands[0]))\n",
    "        self.k_max = int(self.cands[-1])\n",
    "\n",
    "    def _snap(self, k: float) -> int:\n",
    "        best = self.cands[0]\n",
    "        best_d = abs(best - k)\n",
    "        for c in self.cands[1:]:\n",
    "            d = abs(c - k)\n",
    "            if d < best_d:\n",
    "                best, best_d = c, d\n",
    "        return int(best)\n",
    "\n",
    "    def sample(self, update_step: int, epoch: int) -> int:\n",
    "        K_full = int(CONFIG[\"K_max\"])\n",
    "        if (not CONFIG[\"budget_enabled\"]) or (epoch < int(CONFIG[\"warmup_epochs\"])):\n",
    "            return K_full\n",
    "\n",
    "        full_every = int(CONFIG[\"budget_full_every\"])\n",
    "        if full_every > 0 and (update_step % full_every == 0):\n",
    "            return K_full\n",
    "\n",
    "        # log-uniform with bias toward large K\n",
    "        u = self.rng.random()\n",
    "        bias = float(CONFIG.get(\"budget_bias_to_large\", 0.6))\n",
    "        bias = max(1e-6, min(1.0, bias))\n",
    "        u = 1.0 - (1.0 - u) ** (1.0 / bias)\n",
    "\n",
    "        k_cont = math.exp(math.log(self.k_min) + u * (math.log(self.k_max) - math.log(self.k_min)))\n",
    "        k_snap = self._snap(k_cont)\n",
    "        k_snap = max(int(CONFIG[\"budget_k_min\"]), min(self.k_max, k_snap))\n",
    "        return int(k_snap)\n",
    "\n",
    "\n",
    "\n",
    "# 12) Eval\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_lm(model, loader, K_eval=None, max_batches=200, use_amp=True):\n",
    "    model.eval()\n",
    "    total_nll = 0.0\n",
    "    total_tok = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    nb = 0\n",
    "    for batch in loader:\n",
    "        if nb >= int(max_batches):\n",
    "            break\n",
    "        nb += 1\n",
    "        batch = batch.to(CONFIG[\"device\"], non_blocking=True)  # (B, L+1)\n",
    "        x = batch[:, :-1].contiguous()\n",
    "        y = batch[:, 1:].contiguous()\n",
    "\n",
    "        with torch.amp.autocast(device_type=AMP_DEVICE, enabled=(use_amp and CONFIG[\"device\"] == \"cuda\")):\n",
    "            logits, _ = model(x, K_runtime=K_eval)\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1), reduction=\"mean\")\n",
    "\n",
    "        n_tok = y.numel()\n",
    "        total_nll += float(loss.item()) * n_tok\n",
    "        total_tok += int(n_tok)\n",
    "\n",
    "    avg_nll = total_nll / max(1, total_tok)\n",
    "    bpb = avg_nll / math.log(2.0)\n",
    "    ppl = math.exp(avg_nll)\n",
    "\n",
    "    elapsed = max(1e-9, time.time() - t0)\n",
    "    tok_per_s = total_tok / elapsed\n",
    "    return {\"loss_nats\": avg_nll, \"bpb\": bpb, \"ppl\": ppl, \"tok_s\": tok_per_s}\n",
    "\n",
    "@torch.no_grad()\n",
    "def budget_quality_curve(model, val_loader, K_list, max_batches):\n",
    "    print0(\"\\n\" + \"=\" * 72)\n",
    "    print0(f\"[Budgetâ€“Quality] sweep on VAL | batches={max_batches}\")\n",
    "    print0(\"=\" * 72)\n",
    "    for K in K_list:\n",
    "        m = evaluate_lm(model, val_loader, K_eval=int(K), max_batches=max_batches, use_amp=True)\n",
    "        print0(f\"K={int(K):02d} | BPB={m['bpb']:.4f} | PPL={m['ppl']:.2f} | tok/s={m['tok_s']:.0f}\")\n",
    "    print0(\"=\" * 72 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# 13) Save/load training state\n",
    "\n",
    "def state_paths():\n",
    "    base = Path(CONFIG[\"weights_dir\"]) / CONFIG[\"run_name\"]\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    return {\"dir\": base, \"last\": base / \"last_state.pt\", \"best\": base / \"best_state.pt\"}\n",
    "\n",
    "def wipe_run_dir(run_dir: Path):\n",
    "    if run_dir.exists():\n",
    "        for p in run_dir.glob(\"*\"):\n",
    "            try:\n",
    "                if p.is_file() or p.is_symlink():\n",
    "                    p.unlink()\n",
    "                elif p.is_dir():\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def save_state(path: Path, model: nn.Module, optimizer, scheduler, scaler, ema: EMA,\n",
    "              epoch: int, global_step: int, best_bpb: float):\n",
    "    base_model = unwrap_model(model)\n",
    "    payload = {\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"best_bpb\": float(best_bpb),\n",
    "        \"model\": base_model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"ema_shadow\": ema.shadow,\n",
    "        \"config\": CONFIG,\n",
    "    }\n",
    "    tmp = str(path) + \".tmp\"\n",
    "    torch.save(payload, tmp)\n",
    "    os.replace(tmp, str(path))\n",
    "\n",
    "def load_state(path: Path, model: nn.Module, optimizer, scheduler, scaler, ema: EMA):\n",
    "    st = torch.load(path, map_location=\"cpu\")\n",
    "    unwrap_model(model).load_state_dict(st[\"model\"], strict=True)\n",
    "    optimizer.load_state_dict(st[\"optimizer\"])\n",
    "    scheduler.load_state_dict(st[\"scheduler\"])\n",
    "    if scaler is not None and st.get(\"scaler\", None) is not None:\n",
    "        scaler.load_state_dict(st[\"scaler\"])\n",
    "    ema.shadow = st.get(\"ema_shadow\", ema.shadow)\n",
    "    return int(st[\"epoch\"]), int(st[\"global_step\"]), float(st.get(\"best_bpb\", 1e9))\n",
    "\n",
    "\n",
    "\n",
    "# 14) Optim param groups \n",
    "\n",
    "def build_param_groups(model: nn.Module, weight_decay: float):\n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    seen = set()\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if (p is None) or (not p.requires_grad):\n",
    "            continue\n",
    "        pid = id(p)\n",
    "        if pid in seen:\n",
    "            continue\n",
    "        seen.add(pid)\n",
    "\n",
    "        lname = name.lower()\n",
    "        if (p.ndim == 1) or lname.endswith(\"bias\") or (\"norm\" in lname) or (\"embedding\" in lname):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": float(weight_decay)},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "# 15) Train\n",
    "\n",
    "def train():\n",
    "    DEVICE = CONFIG[\"device\"]\n",
    "    paths = state_paths()\n",
    "\n",
    "    if bool(CONFIG.get(\"fresh_start\", False)):\n",
    "        print0(f\"[FreshStart] Wiping run dir: {paths['dir']}\")\n",
    "        wipe_run_dir(paths[\"dir\"])\n",
    "\n",
    "\n",
    "    Nseg = int(CONFIG[\"eval_cache_segments\"])\n",
    "    L = int(CONFIG[\"seq_len\"])\n",
    "    cache_dir = Path(CONFIG[\"eval_cache_dir\"])\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    val_cache_path = cache_dir / f\"pg19_val_L{L}_N{Nseg}.pt\"\n",
    "    test_cache_path = cache_dir / f\"pg19_test_L{L}_N{Nseg}.pt\"\n",
    "\n",
    "    base_seed = int(CONFIG[\"seed\"])\n",
    "    val_seed = base_seed + 111\n",
    "    test_seed = base_seed + 222\n",
    "\n",
    "    val_u16 = build_eval_cache(\n",
    "        split=\"validation\",\n",
    "        out_path=val_cache_path,\n",
    "        num_segments=Nseg,\n",
    "        seed=val_seed,\n",
    "        shuffle_books=bool(CONFIG[\"eval_cache_shuffle_books\"]),\n",
    "        shuffle_buffer_books=int(CONFIG[\"eval_cache_shuffle_buffer_books\"]),\n",
    "        seq_len=L\n",
    "    )\n",
    "    test_u16 = build_eval_cache(\n",
    "        split=\"test\",\n",
    "        out_path=test_cache_path,\n",
    "        num_segments=Nseg,\n",
    "        seed=test_seed,\n",
    "        shuffle_books=bool(CONFIG[\"eval_cache_shuffle_books\"]),\n",
    "        shuffle_buffer_books=int(CONFIG[\"eval_cache_shuffle_buffer_books\"]),\n",
    "        seq_len=L\n",
    "    )\n",
    "\n",
    "\n",
    "    print0(f\"[Sanity] Unigram BPB | VAL={_token_unigram_bpb(val_u16):.4f} | TEST={_token_unigram_bpb(test_u16):.4f}\")\n",
    "    hv = set(_seg_hash_u16(val_u16[i]) for i in range(min(128, val_u16.shape[0])))\n",
    "    ht = set(_seg_hash_u16(test_u16[i]) for i in range(min(128, test_u16.shape[0])))\n",
    "    print0(f\"[Sanity] Segment hash overlap (first 128) | intersection={len(hv.intersection(ht))} (expect 0)\")\n",
    "\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        CachedSegmentsDataset(val_u16),\n",
    "        batch_size=int(CONFIG[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=bool(CONFIG[\"pin_memory\"]),\n",
    "        drop_last=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        CachedSegmentsDataset(test_u16),\n",
    "        batch_size=int(CONFIG[\"batch_size\"]),\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=bool(CONFIG[\"pin_memory\"]),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_shards = _stream_n_shards(\"train\")\n",
    "    train_nw = min(int(CONFIG[\"num_workers\"]), int(train_shards))\n",
    "    print0(f\"[Data] HF n_shards | train={train_shards} val=cache test=cache\")\n",
    "    print0(f\"[Data] DataLoader num_workers capped | train={train_nw} (val/test use cached loaders)\")\n",
    "\n",
    "    train_ds = PG19ByteLMIterable(\n",
    "        split=\"train\",\n",
    "        seq_len=L,\n",
    "        seed=int(CONFIG[\"seed\"]),\n",
    "        shuffle_books=True,\n",
    "        shuffle_buffer_books=int(CONFIG[\"train_shuffle_buffer_books\"]),\n",
    "    )\n",
    "    dl_kw = dict(\n",
    "        batch_size=int(CONFIG[\"batch_size\"]),\n",
    "        num_workers=int(train_nw),\n",
    "        pin_memory=bool(CONFIG[\"pin_memory\"]),\n",
    "        collate_fn=collate_fixed,\n",
    "    )\n",
    "    if train_nw > 0:\n",
    "        dl_kw[\"persistent_workers\"] = True\n",
    "        dl_kw[\"prefetch_factor\"] = 4\n",
    "    train_loader = DataLoader(train_ds, **dl_kw)\n",
    "\n",
    "\n",
    "    print0(\"[Model] Building ES-SSM LM ...\")\n",
    "    model = DeepESSM_LM().to(DEVICE)\n",
    "\n",
    "    if DEVICE == \"cuda\" and torch.cuda.device_count() >= 2:\n",
    "        print0(f\"[DP] Using DataParallel over {torch.cuda.device_count()} GPUs (device_ids=[0,1])\")\n",
    "        model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    else:\n",
    "        print0(f\"[DP] disabled (cuda_count={torch.cuda.device_count()}). Running single GPU/CPU.\")\n",
    "\n",
    "    n_params = sum(p.numel() for p in unwrap_model(model).parameters())\n",
    "    print0(f\"[Model] Params: {n_params/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "    base_model = unwrap_model(model)\n",
    "    param_groups = build_param_groups(base_model, weight_decay=float(CONFIG[\"weight_decay\"]))\n",
    "    opt_kwargs = dict(\n",
    "        lr=float(CONFIG[\"lr\"]),\n",
    "        betas=tuple(CONFIG[\"adam_betas\"]),\n",
    "        eps=float(CONFIG[\"adam_eps\"]),\n",
    "    )\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(param_groups, **opt_kwargs, fused=True)\n",
    "        print0(\"[Opt] Using fused AdamW.\")\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(param_groups, **opt_kwargs)\n",
    "        print0(\"[Opt] Using standard AdamW.\")\n",
    "\n",
    "    scaler = torch.amp.GradScaler(enabled=(bool(CONFIG[\"use_amp\"]) and DEVICE == \"cuda\"))\n",
    "\n",
    "    total_steps = int(CONFIG[\"steps_per_epoch\"]) * int(CONFIG[\"epochs\"])\n",
    "    warmup_steps = int(total_steps * float(CONFIG[\"warmup_ratio\"]))\n",
    "    scheduler = make_warmup_cosine_scheduler(\n",
    "        optimizer, total_steps=total_steps, warmup_steps=warmup_steps, min_lr_ratio=float(CONFIG[\"min_lr_ratio\"])\n",
    "    )\n",
    "    print0(f\"[LR] total_steps={total_steps} | warmup_steps={warmup_steps}\")\n",
    "\n",
    "    ema = EMA(base_model, decay=float(CONFIG[\"ema_decay\"]))\n",
    "    budget = BudgetSampler(int(CONFIG[\"seed\"]))\n",
    "\n",
    "\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    best_bpb = 1e9\n",
    "\n",
    "    if bool(CONFIG.get(\"resume\", False)) and paths[\"last\"].exists():\n",
    "        print0(f\"[Resume] Loading last state: {paths['last']} ...\")\n",
    "        try:\n",
    "            start_epoch, global_step, best_bpb = load_state(paths[\"last\"], model, optimizer, scheduler, scaler, ema)\n",
    "            start_epoch += 1\n",
    "            print0(f\"[Resume] start_epoch={start_epoch} | global_step={global_step} | best_VAL_BPB={best_bpb:.4f}\")\n",
    "        except Exception as e:\n",
    "            print0(f\"[Resume] Incompatible state. Ignoring resume. Error:\\n  {repr(e)}\")\n",
    "            start_epoch, global_step, best_bpb = 0, 0, 1e9\n",
    "\n",
    "    no_improve = 0\n",
    "\n",
    "\n",
    "    train_it_ref = [iter(train_loader)]\n",
    "    def next_batch(it_ref, loader):\n",
    "        try:\n",
    "            return next(it_ref[0])\n",
    "        except StopIteration:\n",
    "            it_ref[0] = iter(loader)\n",
    "            return next(it_ref[0])\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, int(CONFIG[\"epochs\"])):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_nll = 0.0\n",
    "        total_tok = 0\n",
    "        K_meter = []\n",
    "\n",
    "        current_K = budget.sample(update_step=global_step, epoch=epoch)\n",
    "        desc = f\"Epoch {epoch+1:02d}/{CONFIG['epochs']} | ES-SSM | warmup={'Y' if epoch < CONFIG['warmup_epochs'] else 'N'}\"\n",
    "        pbar = tqdm(range(int(CONFIG[\"steps_per_epoch\"]) * int(CONFIG[\"grad_accum_steps\"])), desc=desc)\n",
    "\n",
    "        micro_in_accum = 0\n",
    "        for _ in pbar:\n",
    "            tokens_full = next_batch(train_it_ref, train_loader).to(DEVICE, non_blocking=True)  # (B, L+1)\n",
    "            x = tokens_full[:, :-1].contiguous()\n",
    "            y = tokens_full[:, 1:].contiguous()\n",
    "\n",
    "\n",
    "            p_drop = float(CONFIG.get(\"token_dropout_prob\", 0.0))\n",
    "            if p_drop > 0.0:\n",
    "                drop = (torch.rand_like(x.float()) < p_drop)\n",
    "                x = x.masked_fill(drop, 1)\n",
    "\n",
    "            with torch.amp.autocast(device_type=AMP_DEVICE, enabled=(bool(CONFIG[\"use_amp\"]) and DEVICE == \"cuda\")):\n",
    "                logits, K_used_t = model(x, K_runtime=current_K)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), y.reshape(-1), reduction=\"mean\")\n",
    "                loss = loss / int(CONFIG[\"grad_accum_steps\"])\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            n_tok = y.numel()\n",
    "            total_nll += float(loss.item()) * int(CONFIG[\"grad_accum_steps\"]) * n_tok\n",
    "            total_tok += int(n_tok)\n",
    "\n",
    "            K_meter.append(float(K_used_t.mean().item()))\n",
    "\n",
    "            micro_in_accum += 1\n",
    "            if micro_in_accum >= int(CONFIG[\"grad_accum_steps\"]):\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(CONFIG[\"grad_clip\"]))\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                scheduler.step()\n",
    "                ema.update(unwrap_model(model))\n",
    "\n",
    "                global_step += 1\n",
    "                micro_in_accum = 0\n",
    "                current_K = budget.sample(update_step=global_step, epoch=epoch)\n",
    "\n",
    "            avg_nll = total_nll / max(1, total_tok)\n",
    "            avg_bpb = avg_nll / math.log(2.0)\n",
    "            avg_ppl = math.exp(avg_nll)\n",
    "            avgK = sum(K_meter[-50:]) / max(1, len(K_meter[-50:]))\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"BPB\": f\"{avg_bpb:.4f}\",\n",
    "                \"PPL\": f\"{avg_ppl:.2f}\",\n",
    "                \"avgK\": f\"{avgK:.1f}\",\n",
    "                \"K\": int(current_K),\n",
    "                \"lr\": f\"{lr_now:.2e}\",\n",
    "            })\n",
    "\n",
    "\n",
    "        if micro_in_accum > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), float(CONFIG[\"grad_clip\"]))\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            scheduler.step()\n",
    "            ema.update(unwrap_model(model))\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "        ema.apply(unwrap_model(model))\n",
    "\n",
    "        eval_train_batches = int(CONFIG.get(\"eval_batches_train\", 25))\n",
    "        test_every = int(CONFIG.get(\"test_every\", 3))\n",
    "        curve_every = int(CONFIG.get(\"eval_curve_every\", 3))\n",
    "\n",
    "        val_full = evaluate_lm(\n",
    "            model, val_loader, K_eval=int(CONFIG[\"K_max\"]),\n",
    "            max_batches=eval_train_batches, use_amp=True\n",
    "        )\n",
    "\n",
    "        do_test = (test_every > 0 and ((epoch + 1) % test_every == 0))\n",
    "        if do_test:\n",
    "            test_full = evaluate_lm(\n",
    "                model, test_loader, K_eval=int(CONFIG[\"K_max\"]),\n",
    "                max_batches=eval_train_batches, use_amp=True\n",
    "            )\n",
    "        else:\n",
    "            test_full = {\"bpb\": float(\"nan\"), \"ppl\": float(\"nan\"), \"tok_s\": float(\"nan\")}\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        print0(f\"\\n[Epoch {epoch+1:02d}] time={elapsed:.1f}s | \"\n",
    "               f\"VAL BPB={val_full['bpb']:.4f} PPL={val_full['ppl']:.2f} | \"\n",
    "               f\"TEST BPB={test_full['bpb']:.4f} PPL={test_full['ppl']:.2f}\")\n",
    "\n",
    "        if curve_every > 0 and ((epoch + 1) % curve_every == 0):\n",
    "            budget_quality_curve(\n",
    "                model, val_loader,\n",
    "                K_list=[32, 24, 16, 12, 8, 6, 4, 3, 2],\n",
    "                max_batches=eval_train_batches\n",
    "            )\n",
    "\n",
    "        if int(CONFIG.get(\"save_last_every_epoch\", 1)) > 0:\n",
    "            save_state(paths[\"last\"], model, optimizer, scheduler, scaler, ema, epoch, global_step, best_bpb)\n",
    "\n",
    "        min_delta = float(CONFIG.get(\"early_stop_min_delta\", 1e-4))\n",
    "        improved = (val_full[\"bpb\"] < best_bpb - min_delta)\n",
    "        if improved:\n",
    "            best_bpb = float(val_full[\"bpb\"])\n",
    "            no_improve = 0\n",
    "            save_state(paths[\"best\"], model, optimizer, scheduler, scaler, ema, epoch, global_step, best_bpb)\n",
    "            print0(f\"[Best] New best VAL BPB={best_bpb:.4f} | saved -> {paths['best']}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            print0(f\"[EarlyStop] no_improve={no_improve}/{int(CONFIG['early_stop_patience'])} | best_bpb={best_bpb:.4f}\")\n",
    "\n",
    "        ema.restore(unwrap_model(model))\n",
    "\n",
    "        if no_improve >= int(CONFIG[\"early_stop_patience\"]):\n",
    "            print0(\"[EarlyStop] Triggered. Stop training.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    if paths[\"best\"].exists():\n",
    "        print0(f\"[Final] Loading best state -> {paths['best']}\")\n",
    "        st = torch.load(paths[\"best\"], map_location=\"cpu\")\n",
    "        unwrap_model(model).load_state_dict(st[\"model\"], strict=True)\n",
    "\n",
    "        final_batches = int(CONFIG.get(\"eval_batches_final\", 256))\n",
    "        final_val = evaluate_lm(model, val_loader, K_eval=int(CONFIG[\"K_max\"]), max_batches=final_batches, use_amp=True)\n",
    "        final_test = evaluate_lm(model, test_loader, K_eval=int(CONFIG[\"K_max\"]), max_batches=final_batches, use_amp=True)\n",
    "\n",
    "        print0(\"\\n\" + \"=\" * 72)\n",
    "        print0(f\"[Final Best] VAL BPB={final_val['bpb']:.4f} PPL={final_val['ppl']:.2f} | \"\n",
    "               f\"TEST BPB={final_test['bpb']:.4f} PPL={final_test['ppl']:.2f}\")\n",
    "        print0(\"=\" * 72)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    _ = train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
